import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download, login, HfFolder
import logging

logger = logging.getLogger(__name__)

class ModelHandler:
    """
    GestioneazÄƒ modelul LLM upgraded - foloseÈ™te pattern Singleton È™i suportÄƒ modele mai mari
    UPGRADE: Suport pentru Llama 3.1 8B cu quantization 4-bit pentru eficienÈ›Äƒ RAM
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelHandler, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
            cls._instance.device = None
            cls._instance.initialized = False
            cls._instance.max_context_length = 8192  # Llama 3.1 suportÄƒ mult mai mult context
            cls._instance.model_name = None
            cls._instance.quantized = False
        return cls._instance
    
    def initialize(self, 
                  model_id="meta-llama/Meta-Llama-3.1-8B-Instruct",  # Upgrade la Llama 3.1
                  cache_dir="./model_cache", 
                  use_4bit=True,  # ActiveazÄƒ quantization by default
                  hf_token=None):
        """
        IniÈ›ializeazÄƒ modelul upgraded - suportÄƒ modele mari cu quantization
        
        Args:
            model_id: ID-ul modelului (default: Llama 3.1 8B)
            cache_dir: Directorul de cache
            use_4bit: FoloseÈ™te quantization 4-bit pentru RAM efficiency
            hf_token: Token Hugging Face (necesar pentru unele modele)
        """
        if self.initialized:
            logger.info(f"Modelul {self.model_name} este deja iniÈ›ializat")
            return
        
        try:
            logger.info(f"ğŸš€ Ãncepere iniÈ›ializare model UPGRADED: {model_id}")
            
            # DetecteazÄƒ device-ul disponibil
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ğŸ–¥ï¸  Device detectat: {self.device}")
            
            # AfiÈ™eazÄƒ info despre GPU dacÄƒ e disponibil
            if self.device == "cuda":
                gpu_name = torch.cuda.get_device_name(0)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"ğŸ® GPU: {gpu_name}")
                logger.info(f"ğŸ’¾ Memorie GPU totalÄƒ: {total_memory:.1f} GB")
                
                # VerificÄƒ dacÄƒ avem destulÄƒ memorie pentru modelul mare
                if total_memory < 6 and not use_4bit:
                    logger.warning("âš ï¸  GPU cu <6GB RAM - activez quantization automatÄƒ")
                    use_4bit = True
            else:
                logger.info("âš ï¸  RulÃ¢nd pe CPU - va fi mai lent dar funcÈ›ioneazÄƒ")
            
            # Login la Hugging Face dacÄƒ e necesar
            if hf_token:
                login(token=hf_token)
                logger.info("ğŸ” Autentificare Hugging Face completÄƒ")
            
            # VerificÄƒ È™i creeazÄƒ directorul de cache
            if not os.path.exists(cache_dir):
                logger.info(f"ğŸ“ Creez directorul de cache: {cache_dir}")
                os.makedirs(cache_dir, exist_ok=True)
            
            # ConfigureazÄƒ quantization dacÄƒ e activatÄƒ
            quantization_config = None
            if use_4bit:
                logger.info("âš¡ Configurez quantization 4-bit pentru economie de RAM")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                self.quantized = True
            
            # ÃncÄƒrcare tokenizer
            logger.info("ğŸ“ ÃncÄƒrcare tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                cache_dir=cache_dir,
                trust_remote_code=True
            )
            
            # Setup padding token dacÄƒ nu existÄƒ
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.info("ğŸ”§ Setat padding token")
            
            # ÃncÄƒrcare model cu configuraÈ›ia optimÄƒ
            logger.info(f"ğŸ§  ÃncÄƒrcare model {model_id}...")
            if use_4bit:
                logger.info("   âš¡ Folosind quantization 4-bit - va dura ~2-3 minute")
            else:
                logger.info("   âš¡ ÃncÄƒrcare model complet - va dura ~5-10 minute")
            
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                cache_dir=cache_dir,
                quantization_config=quantization_config,
                device_map="auto" if self.device == "cuda" else None,
                torch_dtype=torch_dtype,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # InformaÈ›ii post-Ã®ncÄƒrcare
            self.model_name = model_id.split('/')[-1]
            
            if self.device == "cuda":
                try:
                    torch.cuda.empty_cache()  # CurÄƒÈ›Äƒm cache-ul GPU
                    allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    cached = torch.cuda.memory_reserved(0) / (1024**3)
                    logger.info(f"ğŸ’¾ Memorie GPU folositÄƒ: {allocated:.1f}GB (cached: {cached:.1f}GB)")
                except:
                    logger.info("ğŸ’¾ Nu pot afiÈ™a info despre memoria GPU")
            
            # EstimeazÄƒ parametrii efectivi
            param_count = sum(p.numel() for p in self.model.parameters()) / 1e9
            logger.info(f"ğŸ”¢ Parametri model: ~{param_count:.1f}B")
            
            if self.quantized:
                logger.info("âš¡ Quantization activÄƒ - modelul foloseÈ™te ~50% mai puÈ›in RAM")
            
            # Test rapid pentru a verifica funcÈ›ionarea
            logger.info("ğŸ§ª Test rapid de funcÈ›ionare...")
            test_result = self._quick_functionality_test()
            
            if test_result:
                self.initialized = True
                logger.info("âœ… Model iniÈ›ializat cu SUCCES!")
                logger.info(f"ğŸ¯ Context maxim: {self.max_context_length} tokens")
                logger.info(f"ğŸš€ Gata pentru analizÄƒ avansatÄƒ de factualitate!")
            else:
                raise Exception("Testul de funcÈ›ionare a eÈ™uat")
                
        except Exception as e:
            logger.error(f"âŒ Eroare la iniÈ›ializarea modelului: {str(e)}")
            self.initialized = False
            raise
    
    def _quick_functionality_test(self):
        """Test rapid pentru a verifica cÄƒ modelul funcÈ›ioneazÄƒ"""
        try:
            test_prompt = "Test: 2+2="
            inputs = self.tokenizer(test_prompt, return_tensors="pt")
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=5,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"ğŸ§ª Test rÄƒspuns: '{response}'")
            return True
            
        except Exception as e:
            logger.error(f"ğŸ§ª Test eÈ™uat: {e}")
            return False
    
    def _truncate_input(self, prompt, max_input_length=6000):  # MÄƒritÄƒ limita pentru Llama 3.1
        """TrunchiazÄƒ input-ul pentru a se Ã®ncadra Ã®n contextul modelului"""
        tokens = self.tokenizer.encode(prompt, add_special_tokens=False)
        
        if len(tokens) <= max_input_length:
            return prompt
        
        logger.warning(f"âš ï¸  Prompt prea lung ({len(tokens)} tokens), trunchiez la {max_input_length}")
        
        # Pentru modele mai puternice, pÄƒstrÄƒm mai mult context
        start_tokens = tokens[:max_input_length//2]
        end_tokens = tokens[-(max_input_length//2):]
        
        start_text = self.tokenizer.decode(start_tokens, skip_special_tokens=True)
        end_text = self.tokenizer.decode(end_tokens, skip_special_tokens=True)
        
        truncated_prompt = start_text + "\n[...text trunchiat pentru analiza optimÄƒ...]\n" + end_text
        return truncated_prompt
    
    def generate_response(self, prompt, max_new_tokens=512, temperature=0.7, do_sample=True):
        """
        GenereazÄƒ rÄƒspuns optimizat pentru Llama 3.1
        
        Args:
            prompt: Promptul de input
            max_new_tokens: NumÄƒrul maxim de tokeni noi
            temperature: Temperatura pentru sampling
            do_sample: DacÄƒ sÄƒ foloseascÄƒ sampling
        """
        if not self.initialized:
            raise RuntimeError("âš ï¸  Modelul nu este iniÈ›ializat! ApeleazÄƒ initialize() mai Ã®ntÃ¢i.")
        
        try:
            # OptimizeazÄƒ promptul pentru Llama 3.1 (foloseÈ™te format chat)
            if "meta-llama/Meta-Llama-3" in self.model_name:
                formatted_prompt = self._format_llama_prompt(prompt)
            else:
                formatted_prompt = prompt
            
            # TrunchiazÄƒ dacÄƒ e necesar
            truncated_prompt = self._truncate_input(formatted_prompt, max_input_length=6000)
            
            logger.info(f"ğŸ”„ Generez rÄƒspuns cu {self.model_name} (primele 50 char): '{truncated_prompt[:50]}...'")
            
            # Tokenizare cu handling Ã®mbunÄƒtÄƒÈ›it
            inputs = self.tokenizer(
                truncated_prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=self.max_context_length - max_new_tokens
            )
            
            # MutÄƒ pe device-ul corect
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            input_length = inputs['input_ids'].shape[1]
            logger.info(f"ğŸ“ Lungime input: {input_length} tokens")
            
            # Generare cu parametri optimizaÈ›i pentru factualitate
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": do_sample,
                "top_p": 0.9,  # Nucleus sampling pentru rÄƒspunsuri mai focalizate
                "repetition_penalty": 1.1,  # EvitÄƒ repetiÈ›iile
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            # Pentru analizÄƒ factualitate, folosim parametri mai conservatori
            if temperature < 0.5:  # Probabil factuality analysis
                generation_config.update({
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "repetition_penalty": 1.05
                })
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)
            
            # DecodeazÄƒ doar partea nouÄƒ
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extrage doar rÄƒspunsul nou (eliminÄƒ promptul)
            if "meta-llama/Meta-Llama-3" in self.model_name:
                response = self._extract_llama_response(full_response, formatted_prompt)
            else:
                response = full_response.replace(truncated_prompt, "").strip()
            
            if not response:
                response = full_response  # Fallback
            
            logger.info(f"âœ… RÄƒspuns generat: {len(response)} caractere")
            logger.info(f"ğŸ” Preview rÄƒspuns: {response[:100]}...")
            
            # CurÄƒÈ›Äƒ memoria GPU dacÄƒ e necesar
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Eroare la generarea rÄƒspunsului: {str(e)}")
            if self.device == "cuda":
                torch.cuda.empty_cache()
            return f"Eroare la procesarea textului cu {self.model_name}: {str(e)[:100]}... Te rog Ã®ncearcÄƒ cu un text mai scurt."
    
    def _format_llama_prompt(self, prompt):
        """FormateazÄƒ promptul pentru Llama 3.1 folosind chat template"""
        messages = [
            {
                "role": "system", 
                "content": "EÈ™ti un asistent AI expert Ã®n verificarea factualitÄƒÈ›ii È™i analiza criticÄƒ a textelor. RÄƒspunde Ã®ntotdeauna Ã®n JSON cÃ¢nd este solicitat."
            },
            {
                "role": "user", 
                "content": prompt
            }
        ]
        
        # FoloseÈ™te chat template dacÄƒ e disponibil
        if hasattr(self.tokenizer, 'apply_chat_template'):
            return self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
        else:
            # Fallback manual pentru Llama format
            formatted = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
            formatted += messages[0]["content"]
            formatted += "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
            formatted += messages[1]["content"]
            formatted += "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            return formatted
    
    def _extract_llama_response(self, full_response, original_prompt):
        """Extrage rÄƒspunsul din output-ul Llama 3.1"""
        # ÃncearcÄƒ sÄƒ gÄƒseascÄƒ rÄƒspunsul dupÄƒ promptul formatat
        if "<|start_header_id|>assistant<|end_header_id|>" in full_response:
            parts = full_response.split("<|start_header_id|>assistant<|end_header_id|>")
            if len(parts) > 1:
                response = parts[-1].strip()
                # EliminÄƒ token-ii de sfÃ¢rÈ™itul rÄƒspunsului
                response = response.replace("<|eot_id|>", "").strip()
                return response
        
        # Fallback: eliminÄƒ promptul original
        return full_response.replace(original_prompt, "").strip()
    
    def get_model_info(self):
        """ReturneazÄƒ informaÈ›ii despre modelul Ã®ncÄƒrcat"""
        if not self.initialized:
            return {"status": "not_initialized"}
        
        return {
            "status": "initialized",
            "model_name": self.model_name,
            "device": self.device,
            "quantized": self.quantized,
            "max_context": self.max_context_length,
            "parameters": f"~{sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B"
        }