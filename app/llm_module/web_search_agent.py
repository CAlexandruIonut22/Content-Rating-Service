# app/llm_module/web_search_agent.py - Optimizat pentru TinyLlama
import sys
import os
import requests
import json
import logging
import re
from typing import Dict, List, Optional

# Fix pentru import-uri
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(os.path.dirname(current_dir))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from .model_handler import ModelHandler

logger = logging.getLogger(__name__)

class WebSearchAgent:
    """
    Agent web search optimizat pentru TinyLlama
    Prompturi simple È™i verificÄƒri rapide
    """
    
    def __init__(self, search_api_key: Optional[str] = None):
        self.search_api_key = search_api_key
        self.search_enabled = search_api_key is not None
        self.model_handler = None
        
        # IniÈ›ializeazÄƒ model handler doar dacÄƒ e necesar
        self._init_model_handler()
        
        logger.info(f"WebSearchAgent iniÈ›ializat (search {'activat' if self.search_enabled else 'dezactivat'})")
    
    def _init_model_handler(self):
        """IniÈ›ializeazÄƒ model handler pentru analizÄƒ"""
        try:
            self.model_handler = ModelHandler()
            if not self.model_handler.initialized:
                # FoloseÈ™te TinyLlama pentru analizÄƒ web
                self.model_handler.initialize(
                    model_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                    use_4bit=False
                )
            logger.info("Model handler pentru web search iniÈ›ializat")
        except Exception as e:
            logger.warning(f"Nu pot iniÈ›ializa model handler pentru web search: {e}")
            self.model_handler = None
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict]:
        """CautÄƒ pe web folosind Tavily API"""
        if not self.search_enabled:
            logger.warning("Web search dezactivat - lipseÈ™te API key")
            return []
        
        try:
            logger.info(f"ðŸ” Caut pe web: '{query}'")
            
            # Tavily Search API
            url = "https://api.tavily.com/search"
            
            payload = {
                "api_key": self.search_api_key,
                "query": query,
                "search_depth": "basic",
                "include_answer": True,
                "include_raw_content": False,
                "max_results": max_results,
                "include_domains": [
                    "wikipedia.org", "britannica.com", "reuters.com", 
                    "bbc.com", "cnn.com", "mediafax.ro", "digi24.ro"
                ],
                "exclude_domains": [
                    "reddit.com", "quora.com", "yahoo.answers.com"
                ]
            }
            
            response = requests.post(url, json=payload, timeout=15)
            response.raise_for_status()
            
            results = response.json()
            search_results = []
            
            for result in results.get('results', []):
                search_results.append({
                    'title': result.get('title', ''),
                    'content': result.get('content', ''),
                    'url': result.get('url', ''),
                    'score': result.get('score', 0)
                })
            
            logger.info(f"âœ… GÄƒsite {len(search_results)} rezultate pentru: {query}")
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ Eroare cÄƒutare web: {e}")
            return []
    
    def extract_claims_for_verification(self, text: str) -> List[str]:
        """
        Extrage afirmaÈ›ii din text pentru verificare
        Versiune simplificatÄƒ pentru TinyLlama
        """
        if not self.model_handler:
            # Fallback simplu fÄƒrÄƒ LLM
            return self._extract_claims_simple(text)
        
        # Prompt foarte simplu pentru TinyLlama
        prompt = f"""CiteÈ™te textul È™i gÄƒseÈ™te 3 lucruri importante care pot fi verificate:

Text: {text[:800]}

Scrie doar 3 afirmaÈ›ii importante, fiecare pe o linie nouÄƒ.
Nu pune numere sau puncte.
Exemplu:
RomÃ¢nia are 19 milioane locuitori
BucureÈ™tiul este capitala RomÃ¢niei
UE a fost creatÄƒ Ã®n 1993"""
        
        try:
            response = self.model_handler.generate_response(
                prompt,
                max_new_tokens=150,
                temperature=0.5,
                do_sample=True
            )
            
            # Extrage afirmaÈ›iile din rÄƒspuns
            claims = []
            lines = response.split('\n')
            
            for line in lines:
                line = line.strip()
                # CurÄƒÈ›Äƒ linia de prefixuri comune
                line = re.sub(r'^[-*â€¢\d+\.)]\s*', '', line)
                
                if len(line) > 15 and len(line) < 200:  # Lungime rezonabilÄƒ
                    claims.append(line)
            
            # LimiteazÄƒ la 3 afirmaÈ›ii maxim
            return claims[:3]
            
        except Exception as e:
            logger.error(f"Eroare extragere afirmaÈ›ii cu TinyLlama: {e}")
            return self._extract_claims_simple(text)
    
    def _extract_claims_simple(self, text):
        """Extragere simplÄƒ de afirmaÈ›ii fÄƒrÄƒ LLM"""
        # ÃŽmparte Ã®n propoziÈ›ii
        sentences = re.split(r'[.!?]', text)
        
        claims = []
        for sentence in sentences:
            sentence = sentence.strip()
            
            # FiltreazÄƒ propoziÈ›iile care par sÄƒ facÄƒ afirmaÈ›ii factuale
            if (len(sentence) > 20 and 
                len(sentence) < 150 and
                any(word in sentence.lower() for word in 
                    ['este', 'sunt', 'a fost', 'au fost', 'are', 'au', 'Ã®n', 'cu', 'de', 'la'])):
                claims.append(sentence)
        
        return claims[:3]  # Maxim 3
    
    def verify_claim_with_search(self, claim: str) -> Dict:
        """VerificÄƒ o afirmaÈ›ie prin cÄƒutare web + TinyLlama"""
        # CreeazÄƒ query simplu pentru cÄƒutare
        search_query = self._simplify_claim_for_search(claim)
        
        # CautÄƒ pe web
        search_results = self.search_web(search_query, max_results=2)
        
        if not search_results:
            return {
                'claim': claim,
                'verification_status': 'no_sources',
                'confidence': 0,
                'explanation': 'Nu s-au gÄƒsit surse pentru verificare',
                'sources_used': []
            }
        
        # PregÄƒteÈ™te context pentru TinyLlama
        context = self._prepare_context_for_tinyllama(search_results)
        
        # VerificÄƒ cu TinyLlama
        return self._verify_with_tinyllama(claim, context, search_results)
    
    def _simplify_claim_for_search(self, claim):
        """SimplificÄƒ afirmaÈ›ia pentru cÄƒutare web"""
        # EliminÄƒ cuvinte comune care nu ajutÄƒ la cÄƒutare
        stop_words = ['este', 'sunt', 'a fost', 'au fost', 'cÄƒ', 'de', 'la', 'Ã®n', 'cu']
        
        words = claim.split()
        filtered_words = [word for word in words if word.lower() not in stop_words]
        
        # PÄƒstreazÄƒ doar primele 6 cuvinte importante
        search_query = ' '.join(filtered_words[:6])
        
        return search_query
    
    def _prepare_context_for_tinyllama(self, search_results):
        """PregÄƒteÈ™te contextul pentru TinyLlama (foarte scurt)"""
        context = ""
        
        for i, result in enumerate(search_results[:2], 1):  # Maxim 2 surse
            title = result.get('title', '')[:100]
            content = result.get('content', '')[:200]  # Foarte scurt pentru TinyLlama
            
            context += f"Sursa {i}: {title}\n{content}\n\n"
        
        return context[:600]  # LimiteazÄƒ contextul total
    
    def _verify_with_tinyllama(self, claim, context, search_results):
        """VerificÄƒ afirmaÈ›ia cu TinyLlama folosind contextul web"""
        if not self.model_handler:
            return self._manual_verification(claim, context, search_results)
        
        # Prompt foarte simplu pentru TinyLlama
        prompt = f"""AfirmaÈ›ia: {claim}

Ce spun sursele de pe internet:
{context}

Este afirmaÈ›ia adevÄƒratÄƒ sau falsÄƒ?
RÄƒspunde doar:
ADEVÄ‚RAT - dacÄƒ sursele confirmÄƒ
FALS - dacÄƒ sursele contrazic  
NECLAR - dacÄƒ nu e destulÄƒ informaÈ›ie

Apoi explicÄƒ pe scurt de ce."""
        
        try:
            response = self.model_handler.generate_response(
                prompt,
                max_new_tokens=100,
                temperature=0.3,
                do_sample=True
            )
            
            # ParseazÄƒ rÄƒspunsul TinyLlama
            verification_result = self._parse_verification_response(response, claim, search_results)
            return verification_result
            
        except Exception as e:
            logger.error(f"Eroare verificare cu TinyLlama: {e}")
            return self._manual_verification(claim, context, search_results)
    
    def _parse_verification_response(self, response, claim, search_results):
        """ParseazÄƒ rÄƒspunsul de verificare de la TinyLlama"""
        response_lower = response.lower()
        
        # DetecteazÄƒ statusul
        if 'adevÄƒrat' in response_lower or 'confirm' in response_lower:
            status = 'adevarata'
            confidence = 7
        elif 'fals' in response_lower or 'contrazic' in response_lower:
            status = 'falsa'
            confidence = 7
        elif 'neclar' in response_lower or 'insuficient' in response_lower:
            status = 'neconcludenta'
            confidence = 4
        else:
            # Fallback bazat pe cuvinte cheie
            positive_words = ['da', 'corect', 'exact', 'confirmat']
            negative_words = ['nu', 'greÈ™it', 'incorect', 'fals']
            
            pos_count = sum(1 for word in positive_words if word in response_lower)
            neg_count = sum(1 for word in negative_words if word in response_lower)
            
            if neg_count > pos_count:
                status = 'falsa'
                confidence = 5
            elif pos_count > neg_count:
                status = 'adevarata'
                confidence = 5
            else:
                status = 'neconcludenta'
                confidence = 3
        
        # Extrage explicaÈ›ia
        explanation = response[:150] + "..." if len(response) > 150 else response
        
        return {
            'claim': claim,
            'verification_status': status,
            'confidence': confidence,
            'explanation': explanation,
            'sources_used': [r['url'] for r in search_results[:2]]
        }
    
    def _manual_verification(self, claim, context, search_results):
        """Verificare manualÄƒ fÄƒrÄƒ LLM"""
        # AnalizÄƒ simplÄƒ bazatÄƒ pe cuvinte cheie
        claim_lower = claim.lower()
        context_lower = context.lower()
        
        # Extrage cuvinte cheie din afirmaÈ›ie
        claim_words = set(re.findall(r'\b\w+\b', claim_lower))
        context_words = set(re.findall(r'\b\w+\b', context_lower))
        
        # CalculeazÄƒ overlap
        overlap = len(claim_words.intersection(context_words))
        total_words = len(claim_words)
        
        if total_words == 0:
            confidence = 0
            status = 'neconcludenta'
        else:
            overlap_ratio = overlap / total_words
            
            if overlap_ratio > 0.6:
                status = 'adevarata'
                confidence = 6
            elif overlap_ratio > 0.3:
                status = 'partial_adevarata'
                confidence = 4
            else:
                status = 'neconcludenta'
                confidence = 3
        
        return {
            'claim': claim,
            'verification_status': status,
            'confidence': confidence,
            'explanation': f'Verificare automatÄƒ: overlap {overlap}/{total_words} cuvinte cu sursele gÄƒsite',
            'sources_used': [r['url'] for r in search_results[:2]]
        }